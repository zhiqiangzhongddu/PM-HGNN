{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "from load_data import load_data\n",
    "from basic_GNN import HGNN\n",
    "from eval_tools import evaluate_results_nc\n",
    "from dqn_agent import DQNAgent\n",
    "from gnn_env import gnn_env\n",
    "from utils import weights_init\n",
    "\n",
    "import torch\n",
    "\n",
    "SEED = random.randint(1,10000001)\n",
    "# SEED = 8684795 # 123 4329\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(SEED)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('SEED', SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = 'IMDB'\n",
    "num_train = 0 # 0 5 10 15 20\n",
    "adam_lr = 0.005 # 0.01 0.001 0.005\n",
    "agent_lr = 1e-4 # 0.01 0.001 0.005\n",
    "weight_decay = 0.0001 # 5e-4, 7e-4\n",
    "batch_size = 4278 # 128 2048 4278\n",
    "hid_dim = 256\n",
    "# state_mode = 2 # 1-node feature 2-unique OH feature\n",
    "agent_batch_coef = 5\n",
    "agent_replay_memory_coef = 100\n",
    "agent_norm_sample = 10\n",
    "agent_action_mode = 1 # 1-action on M nodes, 2-action on All nodes\n",
    "walk_length = 3 # 2, 3, 4, 5, 6, 7\n",
    "max_timesteps = 1 # 1 10\n",
    "max_episodes = 325 # 200 325\n",
    "local_test = False\n",
    "pretrain_gnn = False # 1-True 2-False\n",
    "gnn_type = 'GAT' # GCN GAT NONE \n",
    "gnn_layers = 2\n",
    "agg_type = 2 # 1: agg all steps once; 2: agg by step\n",
    "rnn_type = 'NONE' # gru lstm bi-gru bi-lstm\n",
    "dropout = 0.5 # 0-1\n",
    "act_type = 'relu' # relu elu\n",
    "reward_mode = 'acc p' # dis-eu p acc\n",
    "reward_coef = 100 # 100\n",
    "baseline_experience = 50\n",
    "agent_mlp = [128, 64, 32, 16] # [32, 64, 128, 64, 32]\n",
    "agent_mode = 2 # 0-random 1-by-batch 2-by-item\n",
    "early_stop = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config={'data_name': data_name,\n",
    "              'local_test': local_test,\n",
    "              # 'state_mode': state_mode,\n",
    "              'pretrain_gnn': pretrain_gnn,\n",
    "              'num_train': num_train,\n",
    "              'walk_length': walk_length,\n",
    "              'batch_size': batch_size,\n",
    "              'agent_batch_coef': agent_batch_coef,\n",
    "              'agent_replay_memory_coef': agent_replay_memory_coef,\n",
    "              'agent_norm_sample': agent_norm_sample,\n",
    "              'agent_action_mode': agent_action_mode,\n",
    "              'max_timesteps': max_timesteps,\n",
    "              'max_episodes': max_episodes,\n",
    "              'reward_mode': reward_mode,\n",
    "              'reward_coef': reward_coef,\n",
    "              'baseline_experience': baseline_experience,\n",
    "              'adam_lr': adam_lr, # 1e-2, 5e-3\n",
    "              'agent_lr': agent_lr,\n",
    "              'weight_decay': weight_decay, \n",
    "              # 'num_diff_actions': num_diff_actions,\n",
    "              'hid_dim': hid_dim,\n",
    "              # 'jump_mode': jump_mode,\n",
    "              # 'str_mode': str_mode,\n",
    "              'gnn_type': gnn_type,\n",
    "              'gnn_layers': gnn_layers,\n",
    "              'agg_type': agg_type,\n",
    "              'rnn_type': rnn_type,\n",
    "              'dropout': dropout,\n",
    "              'act_type': act_type,\n",
    "              'agent_mlp': agent_mlp,\n",
    "              'agent_mode': agent_mode,\n",
    "              'early_stop': early_stop,\n",
    "              'SEED': SEED}\n",
    "if torch.cuda.is_available():\n",
    "    model_config['use_cuda'] = True\n",
    "else:\n",
    "    model_config['use_cuda'] = False\n",
    "\n",
    "if num_train == 0:\n",
    "    semisup = True\n",
    "    model_config['semisup'] = True\n",
    "else:\n",
    "    semisup = False\n",
    "    model_config['semisup'] = False\n",
    "\n",
    "print('Parameters of Model are:')\n",
    "for _ in model_config:\n",
    "    print(_, model_config[_])\n",
    "    \n",
    "# set up device\n",
    "device = torch.device('cuda:'+str(0) if model_config['use_cuda'] else 'cpu')\n",
    "model_config['device'] = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data, adj_graph = load_data(\n",
    "    data_name = data_name, num_train = num_train,\n",
    "    local_test = local_test, device = device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HGNN(\n",
    "    pretrain_gnn=pretrain_gnn,\n",
    "    walk_length=walk_length,\n",
    "    num_relations=data.num_relation,\n",
    "    feat_dim=data.x.shape[1],\n",
    "    hid_dim=hid_dim,\n",
    "    out_dim=data.y.unique().shape[0],\n",
    "    gnn_type=gnn_type,\n",
    "    gnn_layers=gnn_layers,\n",
    "    agg_type=agg_type,\n",
    "    rnn_type=rnn_type,\n",
    "    dropout=dropout, \n",
    "    act_type=act_type,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# release gpu memory\n",
    "torch.cuda.empty_cache()\n",
    "model = model.to(device)\n",
    "model.apply(weights_init)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gnn_env(\n",
    "    model, data, model_config, adj_graph, '', device\n",
    ")\n",
    "env.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(\n",
    "    scope='dqn',\n",
    "    action_num=env.action_num,\n",
    "    step_num=env.walk_length,\n",
    "    replay_memory_size=agent_replay_memory_coef * data.val_mask.sum().tolist(), # Size of the replay memory 20000\n",
    "    update_target_estimator_every=50, # 100 1000 # Copy parameters from the Q estimator to the target estimator every N steps 1000\n",
    "    discount_factor=0.99, # Gamma discount factor 0.99\n",
    "    epsilon_start=1.0, # Chance to sample a random action when taking an action. Epsilon is decayed over time and this is the start value 1.0 \n",
    "    epsilon_end=0.05, # The final minimum value of epsilon after decaying is done 0.1\n",
    "    epsilon_decay_steps=max_episodes // 2, # Number of steps to decay epsilon over 20000\n",
    "    batch_size=agent_batch_coef * data.val_mask.sum().tolist(), # 32\n",
    "    norm_sample= agent_norm_sample * data.val_mask.sum().tolist(), # The number of the sample used form noramlize state 100\n",
    "    learning_rate=model_config['agent_lr'], # 0.0005\n",
    "    state_shape = env.observation_space.shape,\n",
    "    mlp_layers=agent_mlp, # [32, 64, 128, 64, 32]\n",
    "    agent_mode=agent_mode, # 0-random 1-by-batch 2-by-item\n",
    "    device=torch.device('cpu')\n",
    ")\n",
    "agent.seed(SEED)\n",
    "env.policy = agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_episode, best_test, best_val = 0, 0, 0\n",
    "\n",
    "# Training: Learning meta-policy\n",
    "print(\"Training Meta-policy on Validation Set\")\n",
    "for i_episode in range(1, max_episodes+1):\n",
    "    print('\\nStart episode {}.'.format(i_episode))\n",
    "    if i_episode == 1:\n",
    "        while agent.total_t < agent.norm_sample:\n",
    "            print('Charging agent normalizer...')\n",
    "            start = time.time()\n",
    "            loss, val_acc, test_acc, test_emb, test_label = agent.learn(env, max_timesteps)\n",
    "        print('Agent normalizer charging is done.\\n')\n",
    "        while len(agent.memory.memory) < agent.memory.batch_size:\n",
    "            print('Charging agent memory...')\n",
    "            loss, val_acc, test_acc, test_emb, test_label = agent.learn(env, max_timesteps)\n",
    "        print('Agent memory charging is done.\\n')\n",
    "        agent.ready_train = True\n",
    "    loss, val_acc, test_acc, test_emb, test_label = agent.learn(env, max_timesteps)\n",
    "    if val_acc > best_val: # check whether gain improvement on validation set\n",
    "        best_policy = deepcopy(agent) # save the best policy\n",
    "        best_val = val_acc\n",
    "        best_test = test_acc\n",
    "        best_episode = i_episode\n",
    "        if semisup:\n",
    "            best_macro, best_micro = evaluate_results_nc(\n",
    "                embeddings=test_emb, \n",
    "                labels=test_label, \n",
    "                num_classes=data.y.unique().shape[0]\n",
    "            )\n",
    "    print(\"Training Meta-policy: {}, DQN Loss:{:.5}, Val_Acc: {:.5f}, Test_Acc: {:.5f}\".\\\n",
    "                  format(i_episode, loss, val_acc, test_acc))    \n",
    "    print(\"Best Episode {}, Val_Acc: {:.5f}, Test_Acc: {:.5f}\".format(\n",
    "        best_episode, best_val, best_test)\n",
    "    )\n",
    "    if semisup:\n",
    "        print('Best Micro-F1: ' + ', '.join(\n",
    "            ['{:.6f}~{:.6f} ({:.1f})'.format(micro_f1_mean, micro_f1_std, train_size) for\n",
    "                        (micro_f1_mean, micro_f1_std), train_size in\n",
    "                        zip(best_micro, [0.8, 0.6, 0.4, 0.2])]\n",
    "        ))\n",
    "        print('Best Macro-F1: ' + ', '.join(\n",
    "            ['{:.6f}~{:.6f} ({:.1f})'.format(macro_f1_mean, macro_f1_std, train_size) for\n",
    "                        (macro_f1_mean, macro_f1_std), train_size in\n",
    "                        zip(best_macro, [0.8, 0.6, 0.4, 0.2])]\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
